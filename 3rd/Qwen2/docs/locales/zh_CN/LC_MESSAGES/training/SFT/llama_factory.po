# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#

msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 19:37+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/training/SFT/llama_factory.rst:2
#: 93f79cd8f3d543e2a9565e714b4c4061
msgid "LLaMA-Factory"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:4
#: d89bbf261ffd4360a3c5d5b0f5a8e784

msgid ""
"Here we provide a script for supervised finetuning Qwen2 with `LLaMA-"
"Factory <https://github.com/hiyouga/LLaMA-Factory>`__. This script for "
"supervised finetuning (SFT) has the following features:"
msgstr ""
"我们将介绍如何使用 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ "
"微调Qwen2模型。本脚本包含如下特点："

#: ../../source/training/SFT/llama_factory.rst:8
#: dbd769a8c23b40c48dabc911f58c10bd
msgid "Support single-GPU and multi-GPU training;"
msgstr "支持单卡和多卡分布式训练"

#: ../../source/training/SFT/llama_factory.rst:10
#: b767e5162d9f42b69a69c81bcbf6a91a
msgid "Support full-parameter tuning, LoRA, Q-LoRA, Dora."
msgstr "支持全参数微调、LoRA、Q-LoRA 和 DoRA 。"

#: ../../source/training/SFT/llama_factory.rst:12
#: 3483351614d24e8c88f9ca463eac8350
msgid "In the following, we introduce more details about the usage of the script."
msgstr "下文将介绍更多关于脚本的用法。"

#: ../../source/training/SFT/llama_factory.rst:16
#: 02e4e3a0004e4aa98537a0eab7cf5463
msgid "Installation"
msgstr "安装"

#: ../../source/training/SFT/llama_factory.rst:18
#: e4e633744c1a49a993d6e422150c36f6
msgid "Before you start, make sure you have installed the following packages:"
msgstr "开始之前，确保你已经安装了以下代码库："

#: ../../source/training/SFT/llama_factory.rst:20
#: ad671c469cb543fcb05e2d2d5b9e3e21
msgid ""
"Follow the instructions of `LLaMA-Factory <https://github.com/hiyouga"
"/LLaMA-Factory>`__, and build the environment."
msgstr ""
"根据 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ "
"官方指引构建好你的环境"

#: ../../source/training/SFT/llama_factory.rst:23
#: 2937fd8145ca4728a3b541c229a29fcc
msgid "Install these packages (Optional):"
msgstr "安装下列代码库（可选）："

#: ../../source/training/SFT/llama_factory.rst:30
#: ca09bf3473fd465f97592b84aad04e1c
msgid ""
"If you want to use `FlashAttention-2 <https://github.com/Dao-AILab/flash-"
"attention>`__, make sure your CUDA is 11.6 and above."
msgstr ""
"如你使用 `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__"
"  ，请确保你的CUDA版本在11.6以上。"

#: ../../source/training/SFT/llama_factory.rst:35
#: 6da4e0f20fec44799a0cc8015adcc3e5
msgid "Data Preparation"
msgstr "准备数据"

#: ../../source/training/SFT/llama_factory.rst:37
#: 4cd4de7f0ae645c7b25c8222b8ffd962
msgid ""
"LLaMA-Factory provides several training datasets in ``data`` folder, you "
"can use it directly. If you are using a custom dataset, please prepare "
"your dataset as follow."
msgstr ""
"LLaMA-Factory 在 ``data`` "
"文件夹中提供了多个训练数据集，您可以直接使用它们。如果您打算使用自定义数据集，请按照以下方式准备您的数据集。"

#: ../../source/training/SFT/llama_factory.rst:41
#: b8e282da098641cd998d2fa8c07351a7
msgid ""
"Organize your data in a **json** file and put your data in ``data`` "
"folder. LLaMA-Factory supports dataset in ``alpaca`` or ``sharegpt`` "
"format."
msgstr ""
"请将您的数据以 ``json`` 格式进行组织，并将数据放入 data 文件夹中。LLaMA-Factory 支持以 ``alpaca`` 或 "
"``sharegpt`` 格式的数据集。"

#: ../../source/training/SFT/llama_factory.rst:45
#: 77e3ab3b1bf34e218d96ea6a09ae208e
msgid "The dataset in ``alpaca`` format should follow the below format:"
msgstr "``alpaca`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:62
#: a28a06cfddcb451d8d982d8bb2cb0efb
msgid "The dataset in ``sharegpt`` format should follow the below format:"
msgstr "``sharegpt`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:83
#: a97642500c69421ebd4d0c0721aac9e9
msgid ""
"Provide your dataset definition in ``data/dataset_info.json`` in the "
"following format ."
msgstr "在 ``data/dataset_info.json`` 文件中提供您的数据集定义，并采用以下格式："

#: ../../source/training/SFT/llama_factory.rst:86
#: 740d53d75a9f4b509cb2f774846ceca5
msgid ""
"For ``alpaca`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``alpaca`` 格式的数据集，其 ``dataset_info.json`` 文件中的列应为："

#: ../../source/training/SFT/llama_factory.rst:102
#: 54563fc901764dc7918589c297eb17bd
msgid ""
"For ``sharegpt`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``sharegpt`` 格式的数据集，``dataset_info.json`` 文件中的列应该包括："

#: ../../source/training/SFT/llama_factory.rst:124
#: 32d6014b340847cebf07b91d7a024272
msgid "Training"
msgstr "训练"

#: ../../source/training/SFT/llama_factory.rst:126
#: f5145b2c38da4f13b256681e1ed73912
msgid "Execute the following training command:"
msgstr "执行下列命令："

#: ../../source/training/SFT/llama_factory.rst:166
#: 5977b71f51cd43c7bc2e204ef5912bf0
msgid ""
"and enjoy the training process. To make changes to your training, you can"
" modify the arguments in the training command to adjust the "
"hyperparameters. One argument to note is ``cutoff_len``, which is the "
"maximum length of the training data. Control this parameter to avoid OOM "
"error."
msgstr ""
"并享受训练过程。若要调整您的训练，您可以通过修改训练命令中的参数来调整超参数。其中一个需要注意的参数是 ``cutoff_len`` "
"，它代表训练数据的最大长度。通过控制这个参数，可以避免出现OOM（内存溢出）错误。"

#: ../../source/training/SFT/llama_factory.rst:173
#: f7866d01219c41d19f753e697d0d8eae
msgid "Merge LoRA"
msgstr "合并LoRA"

#: ../../source/training/SFT/llama_factory.rst:175
#: d4aa6396f573444a8bb3b12df9ff0661
msgid ""
"If you train your model with LoRA, you probably need to merge adapter "
"parameters to the main branch. Run the following command to perform the "
"merging of LoRA adapters."
msgstr "如果你使用 LoRA 训练模型，可能需要将adapter参数合并到主分支中。请运行以下命令以执行 LoRA adapter 的合并操作。"

#: ../../source/training/SFT/llama_factory.rst:191
#: 3a5652c17cf54ddf957414b3b4d886b0
msgid "Conclusion"
msgstr "结语"

#: ../../source/training/SFT/llama_factory.rst:193
#: 01bf0d3fbaff4c81bc3bb8ea218fad88
msgid ""
"The above content is the simplest way to use LLaMA-Factory to train Qwen."
" Feel free to dive into the details by checking the official repo!"
msgstr "上述内容是使用LLaMA-Factory训练Qwen的最简单方法。 欢迎通过查看官方仓库深入了解详细信息！"

