# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#

msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 19:37+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/training/SFT/example.rst:2 bc6b4b30124a414b83ef39c8e3ae7cd5
msgid "Example"
msgstr "示例"

#: ../../source/training/SFT/example.rst:4 b075974261cb4a15b87b20bb208d80af

msgid ""
"Here we provide a very simple script for supervised finetuning, which is "
"revised from the training script in `Fastchat <https://github.com/lm-"
"sys/FastChat>`__. The script is used to finetune Qwen with Hugging Face "
"Trainer. You can check the script `here "
"<https://github.com/QwenLM/Qwen2/blob/main/finetune.py>`__. This script "
"for supervised finetuning (SFT) has the following features:"
msgstr ""
"在这里，我们提供了一个非常简单的有监督微调脚本，该脚本是基于 `Fastchat <https://github.com/lm-"
"sys/FastChat>`__ 的训练脚本修改而来的。这个脚本用于使用Hugging Face Trainer "
"对Qwen模型进行微调。你可以在以下链接查看这个脚本： `这里 "
"<https://github.com/QwenLM/Qwen2/blob/main/finetune.py>`__ 。这个脚本具有以下特点："

#: ../../source/training/SFT/example.rst:11 064901651ff6430194d25b5593d48877
msgid "Support single-GPU and multi-GPU training;"
msgstr "支持单卡和多卡分布式训练"

#: ../../source/training/SFT/example.rst:12 1629ddb7bc0d4766b5d2dcd6f644d579
msgid ""
"Support full-parameter tuning, `LoRA "
"<https://arxiv.org/abs/2106.09685>`__, and `Q-LoRA "
"<https://arxiv.org/abs/2305.14314>`__."
msgstr ""
"支持全参数微调、 `LoRA <https://arxiv.org/abs/2106.09685>`__ 以及 `Q-LoRA "
"<https://arxiv.org/abs/2305.14314>`__  。"

#: ../../source/training/SFT/example.rst:16 42e112b0ffb54140a79a6e17465a1aaa
msgid "In the following, we introduce more details about the usage of the script."
msgstr "下面，我们介绍脚本的更多细节。"

#: ../../source/training/SFT/example.rst:20 9bd5c830bbef46f1a969a2c8e6c168cf
msgid "Installation"
msgstr "安装"

#: ../../source/training/SFT/example.rst:22 7dd9650264534a3eab9dd9c901161ea3
msgid "Before you start, make sure you have installed the following packages:"
msgstr "开始之前，确保你已经安装了以下代码库："

#: ../../source/training/SFT/example.rst:29 83e6e9b0a97d42c8bf541798b788eadf
msgid "Data Preparation"
msgstr "准备数据"

#: ../../source/training/SFT/example.rst:31 ffb32c0d97e84fe1b6ec22dc958a60e1
msgid ""
"For data preparation, we advise you to organize the data in a jsonl file,"
" where each line is a dictionary as demonstrated below:"
msgstr "我们建议你放在jsonl文件中，每一行是一个如下所示的字典："

#: ../../source/training/SFT/example.rst:76 ac1d7e0f0ea7495ab246618e8bf7f342
msgid ""
"Above are two examples of each data sample in the dataset. Each sample is"
" a JSON object with the following fields: ``type``, ``messages`` and "
"``source``. ``messages`` is required while the others are optional for "
"you to label your data format and data source. The ``messages`` field is "
"a list of JSON objects, each of which has two fields: ``role`` and "
"``content``. ``role`` can be ``system``, ``user``, or ``assistant``. "
"``content`` is the text of the message. ``source`` is the source of the "
"data, which can be ``self-made``, ``alpaca``, ``open-hermes``, or any "
"other string."
msgstr ""
"以上提供了该数据集中的每个样本的两个示例。每个样本都是一个JSON对象，包含以下字段： ``type`` 、 ``messages`` 和 "
"``source`` 。其中， ``messages`` 是必填字段，而其他字段则是供您标记数据格式和数据来源的可选字段。 "
"``messages`` 字段是一个JSON对象列表，每个对象都包含两个字段： ``role`` 和 ``content`` 。其中， "
"``role`` 可以是 ``system`` 、 ``user`` 或 ``assistant`` ，表示消息的角色； ``content`` "
"则是消息的文本内容。而 ``source`` 字段代表了数据来源，可能包括 ``self-made`` 、 ``alpaca`` 、 "
"``open-hermes`` 或其他任意字符串。"

#: ../../source/training/SFT/example.rst:86 affe60c4a49547eda76b4081eba0b2d6
msgid ""
"To make the jsonl file, you can use ``json`` to save a list of "
"dictionaries to the jsonl file:"
msgstr "你需要用 ``json`` 将一个字典列表存入jsonl文件中："

#: ../../source/training/SFT/example.rst:98 8924a0ebdaf54235b404cdfccccf297d
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/training/SFT/example.rst:100 f65e1f9f8136444e8677d1c22a546f1c
msgid ""
"For you to start finetuning quickly, we directly provide a shell script "
"for you to run without paying attention to details. You need different "
"hyperparameters for different types of training, e.g., single-GPU / "
"multi-GPU training, full-parameter tuning, LoRA, or Q-LoRA."
msgstr ""
"为了让您能够快速开始微调，我们直接提供了一个shell脚本，您可以无需关注具体细节即可运行。针对不同类型的训练（例如单GPU训练、多GPU训练、全参数微调"
"、LoRA或Q-LoRA），您可能需要不同的超参数设置。"

#: ../../source/training/SFT/example.rst:113 3e55a646f63547b9afa9f9461be4b013
msgid ""
"Specify the ``<model_path>`` for your model, ``<data_path>`` for your "
"data, and ``<config_path>`` for your deepspeed configuration. If you use "
"LoRA or Q-LoRA, just add ``--use_lora True`` or ``--q_lora True`` based "
"on your requirements. This is the simplest way to start finetuning. If "
"you want to change more hyperparameters, you can dive into the script and"
" modify those parameters."
msgstr ""
"为您的模型指定 ``<model_path>`` ，为您的数据指定 ``<data_path>`` ，并为您的Deepspeed配置指定 "
"``<config_path>`` 。如果您使用LoRA或Q-LoRA，只需根据您的需求添加 ``--use_lora True`` 或 "
"``--q_lora True`` 。这是开始微调的最简单方式。如果您想更改更多超参数，您可以深入脚本并修改这些参数。"

#: ../../source/training/SFT/example.rst:122 2004802217c34285a9eb803444495910
msgid "Advanced Usages"
msgstr "高级用法"

#: ../../source/training/SFT/example.rst:124 9565f7ed6c0443c6b0c2a69b7776fab0
msgid ""
"In this section, we introduce the details of the scripts, including the "
"core python script as well as the corresponding shell script."
msgstr "在这个部分中，我们介绍python脚本以及shell脚本的相关细节"

#: ../../source/training/SFT/example.rst:128 6cd4e1bfa76e4eec8217aa23b707ab37
msgid "Shell Script"
msgstr "Shell脚本"

#: ../../source/training/SFT/example.rst:130 986e82567e0b41ecbbf2be8de2b0ddca
msgid ""
"Before we introduce the python code, we provide a brief introduction to "
"the shell script with commands. We provide some guidance inside the shell"
" script and here we take ``finetune.sh`` as an example."
msgstr ""
"在展示Python代码之前，我们先对包含命令的Shell脚本做一个简单的介绍。我们在Shell脚本中提供了一些指南，并且此处将以 "
"``finetune.sh`` 这个脚本为例进行解释说明。"

#: ../../source/training/SFT/example.rst:134 a30125285d9f42d3b6b4d7095e3ebaa7
msgid ""
"To set up the environment variables for distributed training (or single-"
"GPU training), specify the following variables: ``GPUS_PER_NODE``, "
"``NNODES``, ``NODE_RANK``, ``MASTER_ADDR``, and ``MASTER_PORT``. No need "
"to worry too much about them as we provide the default settings for you. "
"In the command, you can pass in the argument ``-m`` and ``-d`` to specify"
" the model path and data path, respectively. You can also pass in the "
"argument ``--deepspeed`` to specify the deepspeed configuration file. We "
"provide two configuration files for ZeRO2 and ZeRO3, and you can choose "
"one based on your requirements. In most cases, we recommend using ZeRO3 "
"for multi-GPU training except for Q-LoRA, where we recommend using ZeRO2."
msgstr ""
"要为分布式训练（或单GPU训练）设置环境变量，请指定以下变量： ``GPUS_PER_NODE`` 、 ``NNODES、NODE_RANK`` "
"、 ``MASTER_ADDR`` 和 ``MASTER_PORT`` "
"。不必过于担心这些变量，因为我们为您提供了默认设置。在命令行中，您可以通过传入参数 ``-m`` 和 ``-d`` "
"来分别指定模型路径和数据路径。您还可以通过传入参数 ``--deepspeed`` "
"来指定Deepspeed配置文件。我们为您提供针对ZeRO2和ZeRO3的两种配置文件，您可以根据需求选择其中之一。在大多数情况下，我们建议在多GPU训练中使用ZeRO3"
"，但针对Q-LoRA，我们推荐使用ZeRO2。"

#: ../../source/training/SFT/example.rst:146 af082f1fbc3d4ae2bcf24d077fa32940
msgid ""
"There are a series of hyperparameters to tune. Passing in ``--bf16`` or "
"``--fp16`` to specify the precision for mixed precision training. The "
"other significant hyperparameters include:"
msgstr ""
"有一系列需要调节的超参数。您可以向程序传递 ``--bf16`` 或 ``--fp16`` "
"参数来指定混合精度训练所采用的精度级别。此外，还有其他一些重要的超参数如下："

#: ../../source/training/SFT/example.rst:150 951a577805b7460e9298d40406a43814
msgid "``--output_dir``: the path of your output models or adapters."
msgstr " ``--output_dir`` ：模型或者adapter的输出路径；."

#: ../../source/training/SFT/example.rst:151 38eefeba82c44bbf9c9f18e67ea2e5c8
msgid "``--num_train_epochs``: the number of training epochs."
msgstr " ``--num_train_epochs`` : 训练轮数；"

#: ../../source/training/SFT/example.rst:152 ead297f1fde64ee5a0fe58ad93ede7d9
msgid ""
"``--gradient_accumulation_steps``: the number of gradient accumulation "
"steps."
msgstr " ``--gradient_accumulation_steps`` ：梯度累积；"

#: ../../source/training/SFT/example.rst:154 99d7b6582e6d4ec0bf0d60c082b50cd5
msgid ""
"``--per_device_train_batch_size``: the batch size per GPU for training, "
"and the total batch size is equalt to ``per_device_train_batch_size`` "
":math:`\\times` ``number_of_gpus`` :math:`\\times` "
"``gradient_accumulation_steps``."
msgstr ""
" ``--per_device_train_batch_size`` 参数表示每个GPU在训练时的批次大小，而总批次大小等于 "
"``per_device_train_batch_size`` 乘以 GPU 的数量，再乘以 "
"``gradient_accumulation_steps`` ；"

#: ../../source/training/SFT/example.rst:158 90ab9e448ee94924a52d3dd1f63af623
msgid "``--learning_rate``: the learning rate."
msgstr " ``--learning_rate`` ：学习率；"

#: ../../source/training/SFT/example.rst:159 022db84b694342fd9983e42abc1a35db
msgid "``--warmup_steps``: the number of warmup steps."
msgstr " ``--warmup_steps`` ：warmup的步数；"

#: ../../source/training/SFT/example.rst:160 3c89749851af4f5a99f707bd59f1801d
msgid "``--lr_scheduler_type``: the type of learning rate scheduler."
msgstr " ``--lr_scheduler_type`` ：learning rate scheduler类型；"

#: ../../source/training/SFT/example.rst:161 42387b7665734a7fb2d25e048db10551
msgid "``--weight_decay``: the value of weight decay."
msgstr " ``--weight_decay``：weight decay的值；"

#: ../../source/training/SFT/example.rst:162 ebc53b476c514dce903b9234fe4eae8f
msgid "``--adam_beta2``: the value of :math:`\\beta_2` in Adam."
msgstr " ``--adam_beta2`` ：Adam :math:`\\beta_2` "

#: ../../source/training/SFT/example.rst:163 914b695fc866428a96d1e2bc2457611e
msgid "``--model_max_length``: the maximum sequence length."
msgstr " ``--model_max_length`` ：最大长度；"

#: ../../source/training/SFT/example.rst:164 473da921cb97469dadd579c7f9e436a3
msgid ""
"``--use_lora``: whether to use LoRA. Adding ``--q_lora`` can enable "
"Q-LoRA."
msgstr " ``--use_lora`` ：是否使用LoRA，加上 ``--q_lora`` 即可启用Q-LoRA。\""

#: ../../source/training/SFT/example.rst:166 48d9fd71fcc14412aa5bcf54a1ef5180
msgid "``--gradient_checkpointing``: whether to use gradient checkpointing."
msgstr " ``--gradient_checkpointing`` ：是否使用gradient checkpointing；"

#: ../../source/training/SFT/example.rst:169 bfdf4ab6c98447c0a441b9796cd61397
msgid "Python Script"
msgstr "Python脚本"

#: ../../source/training/SFT/example.rst:171 4b2c15df84ae4157aafcf57d43496a1a
msgid ""
"In this script, we mainly use ``trainer`` from HF and ``peft`` to train "
"our models. We also use ``deepspeed`` to accelerate the training process."
" The script is very simple and easy to understand."
msgstr ""
"在本脚本中，我们主要使用来自HF的 ``trainer`` 和 ``peft`` 来训练我们的模型。同时，我们也利用 ``deepspeed`` "
"来加速训练过程。该脚本非常简洁且易于理解。"

#: ../../source/training/SFT/example.rst:227 4bc5f93db9254ed1a34c9f85de78794b
msgid ""
"The classes for arguments allow you to specify hyperparameters for model,"
" data, training, and additionally LoRA if you use LoRA or Q-LoRA to train"
" your model. Specifically, ``model-max-length`` is a key hyperparameter "
"that determines your maximum sequence length of your training data."
msgstr ""
"参数类允许你为模型、数据和训练指定超参数，如果使用LoRA或Q-LoRA训练模型，还会包含这两个方法的相关超参数。具体来说， ``model-"
"max-length`` 是一个关键的超参数，它决定了训练数据的最大序列长度。"

#: ../../source/training/SFT/example.rst:233 6608d5246c2c456d856c55df3916a386
msgid "``LoRAArguments`` includes the hyperparameters for LoRA or Q-LoRA:"
msgstr ""

#: ../../source/training/SFT/example.rst:235 5821bf9d95654967997fc81057238b29
msgid "``lora_r``: the rank for LoRA;"
msgstr " ``LoRAArguments`` 包含了用于LoRA或Q-LoRA的超参数："

#: ../../source/training/SFT/example.rst:236 8c9543de5e8b4935b103496fd60ac554
msgid "``lora_alpha``: the alpha value for LoRA;"
msgstr " `lora_alpha`` ：LoRA的alpha值；"

#: ../../source/training/SFT/example.rst:237 8428521a315b4ff390e3965d0da2c489
msgid "``lora_dropout``: the dropout rate for LoRA;"
msgstr " ``lora_dropout`` ：LoRA的Dropout；"

#: ../../source/training/SFT/example.rst:238 b8555e14abf84fcca82c98ed95180b92
msgid ""
"``lora_target_modules``: the target modules for LoRA. By default we tune "
"all linear layers;"
msgstr " ``lora_target_modules`` ：LoRA的目标模块，默认情况下，我们对所有线性层进行调优；"

#: ../../source/training/SFT/example.rst:240 a48f7902cb1544eda149491a0c0d6a2e
msgid "``lora_weight_path``: the path to the weight file for LoRA;"
msgstr " ``lora_weight_path``：LoRA的权重路径；"

#: ../../source/training/SFT/example.rst:241 66fbbb60c99f4cc6a3646a2b38a09bb1
msgid "``lora_bias``: the bias for LoRA;"
msgstr " ``lora_bias`` ：LoRA的bias"

#: ../../source/training/SFT/example.rst:242 549e076ff1ab4300a6ba7fa0fbaf16a3
msgid "``q_lora``: whether to use Q-LoRA."
msgstr " ``q_lora`` 是否使用Q-LoRA。\""

#: ../../source/training/SFT/example.rst:300 6986b17e0b2244d2bf540367853e2eec
msgid ""
"The method ``safe_save_model_for_hf_trainer``, which uses "
"``get_peft_state_maybe_zero_3``, helps tackle the problems in saving "
"models trained either with or without ZeRO3."
msgstr ""
"方法 ``safe_save_model_for_hf_trainer`` 通过使用 "
"``get_peft_state_maybe_zero_3`` 有助于解决在保存采用或未采用ZeRO3技术训练的模型时遇到的问题。"

#: ../../source/training/SFT/example.rst:334 331f116ce2b549189a93d916f84b5f30
msgid ""
"For data preprocessing, we use ``preprocess`` to organize the data. "
"Specifically, we apply our ChatML template to the texts. If you prefer "
"other chat templates, you can use others, e.g., by still applying "
"``apply_chat_template()`` with another tokenizer. The chat template is "
"stored in the ``tokenizer_config.json`` in the HF repo. Additionally, we "
"pad the sequence of each sample to the maximum length for training."
msgstr ""
"在数据预处理阶段，我们使用 ``preprocess`` "
"来整理数据。具体来说，我们应用ChatML模板对文本进行处理。如果您倾向于使用其他chat模板，您也可以选择其他的，例如，仍然通过``apply_chat_template()``函数配合另一个tokenizer进行应用。Chat模板存储在HF仓库中的"
" ``tokenizer_config.json`` 文件中。此外，我们还将每个样本的序列填充到最大长度，以便于训练。"

#: ../../source/training/SFT/example.rst:431 1655d97fbb504ca1a3c57b0c772a12b3
msgid ""
"Then we utilize ``make_supervised_data_module`` by using "
"``SupervisedDataset`` or ``LazySupervisedDataset`` to build the dataset."
msgstr ""
"然后我们利用 ``make_supervised_data_module`` ，通过使用 ``SupervisedDataset`` 或 "
"``LazySupervisedDataset`` 来构建数据集。"

#: ../../source/training/SFT/example.rst:554 898add3fc9974cd1ac5933d941ddfe9e
msgid ""
"The ``train`` method is the key to the training. In general, it loads the"
" tokenizer and model with ``AutoTokenizer.from_pretrained()`` and "
"``AutoModelForCausalLM.from_pretrained()``. If we use LoRA, the method "
"will initialize LoRA configuration with ``LoraConfig``. If we apply "
"Q-LoRA, we should use ``prepare_model_for_kbit_training``. Note that for "
"now it still does not support resume for LoRA. Then we leave the "
"following efforts to ``trainer`` and have a cup of coffee!"
msgstr ""
"``train`` 方法是训练过程的关键所在。通常情况下，它会通过 ``AutoTokenizer.from_pretrained()`` 和 "
"``AutoModelForCausalLM.from_pretrained()`` "
"来加载tokenizer和模型。如果使用了LoRA，该方法将会用 ``LoraConfig`` 初始化LoRA配置。若应用Q-LoRA，则应当采用"
" ``prepare_model_for_kbit_training`` "
"。请注意，目前还不支持对LoRA的续训（resume）。接下来的任务就交给trainer处理，此时不妨喝杯咖啡稍作休息！"

#: ../../source/training/SFT/example.rst:563 7f4b5671c9734aacb7d46436184c6275
msgid "Next Step"
msgstr "下一步"

#: ../../source/training/SFT/example.rst:565 4506c0fdf6b54185a5ea1ee46719adb4
msgid ""
"Now, you are able to use a very simple script to perform different types "
"of SFT. Alternatively, you can use more advanced training libraries, such"
" as `Axolotl <https://github.com/OpenAccess-AI-Collective/axolotl>`__ or "
"`LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__, to enjoy "
"more functionalities. To take a step forward, after SFT, you can consider"
" RLHF to align your model to human preferences! Stay tuned for our next "
"tutorial on RLHF!"
msgstr ""
"现在，您可以使用一个非常简单的脚本来执行不同类型的SFT。另外，您还可以选择使用更高级的训练库，例如 `Axolotl "
"<https://github.com/OpenAccess-AI-Collective/axolotl>`__ 或 `LLaMA-Factory"
" <https://github.com/hiyouga/LLaMA-Factory>`__ "
"，以享受更多的功能。进一步来说，在完成SFT之后，您可以考虑采用RLHF（强化学习与人类反馈）来使您的模型更好地与人类偏好对齐！请继续关注我们接下来关于RLHF的教程！"

