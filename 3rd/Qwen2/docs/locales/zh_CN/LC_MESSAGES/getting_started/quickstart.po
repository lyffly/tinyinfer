# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-07-18 20:15+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/getting_started/quickstart.rst:2
#: 6ed10ddb9f504706a246be5e9d58774e
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/getting_started/quickstart.rst:4
#: 72aa8b61e4cf478fa26bdd1a138d40e5
msgid ""
"This guide helps you quickly start using Qwen2. We provide examples of "
"`Hugging Face Transformers "
"<https://github.com/huggingface/transformers>`__ as well as `ModelScope "
"<https://github.com/modelscope/modelscope>`__, and `vLLM "
"<https://github.com/vllm-project/vllm>`__ for deployment."
msgstr ""
"本指南帮助您快速上手Qwen2的使用，并提供了如下示例： `Hugging Face Transformers "
"<https://github.com/huggingface/transformers>`__ 以及 `ModelScope "
"<https://github.com/modelscope/modelscope>`__ 和 `vLLM <https://github.com"
"/vllm-project/vllm>`__ 在部署时的应用实例。"

#: ../../source/getting_started/quickstart.rst:9
#: 286afb554d0447bba27cbcf859b02137
msgid ""
"You can find Qwen2 models in the `Qwen2 collection "
"<https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f>`__."
msgstr ""
"你可以在 `Qwen2 collection "
"<https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f>`__"
" 发现 Qwen2 模型"

#: ../../source/getting_started/quickstart.rst:12
#: 24889daa5472428b95b655206995ea2f
msgid "Hugging Face Transformers & ModelScope"
msgstr "Hugging Face Transformers & ModelScope"

#: ../../source/getting_started/quickstart.rst:14
#: f410a917f57e4789bfbcfb92673780d0
msgid ""
"To get a quick start with Qwen2, we advise you to try with the inference "
"with ``transformers`` first. Make sure that you have installed "
"``transformers>=4.40.0``. We advise you to use Python 3.8 or higher, and "
"Pytorch 2.2 or higher."
msgstr ""
"要快速上手 Qwen2 ，我们建议您首先尝试使用 ``transformers`` 进行推理。请确保已安装了 "
"``transformers>=4.40.0`` 版本。我们建议您使用 Python 3.8 或以上版本， PyTorch 2.2 或以上版本。"

#: ../../source/getting_started/quickstart.rst:18
#: 7017e7a2eb3843738fd53b295ca08212
msgid "Install with ``pip``:"
msgstr "使用 ``pip`` 安装："

#: ../../source/getting_started/quickstart.rst:24
#: c0d405e3b1e744948f556020c2ff524d
msgid "Install with ``conda``:"
msgstr "使用 ``conda`` 安装："

#: ../../source/getting_started/quickstart.rst:30
#: cd3c1e70ddb041dda2a72d6d99ba88c4
msgid "Install from source:"
msgstr "从源代码安装："

#: ../../source/getting_started/quickstart.rst:37
#: f410a917f57e4789bfbcfb92673780d0
msgid ""
"The following is a very simple code snippet showing how to run "
"Qwen2-Instruct, with an example of Qwen2-7B-Instruct:"
msgstr "以下是一个非常简单的代码片段示例，展示如何运行Qwen2-Instruct模型，其中包含 ``Qwen2-7B-Instruct`` 的实例："

#: ../../source/getting_started/quickstart.rst:78
#: 265eeb20765743e5bcff264d4f3ea5bf
msgid ""
"Previously, we use ``model.chat()`` (see ``modeling_qwen.py`` in previous"
" Qwen models for more information). Now, we follow the practice of "
"``transformers`` and directly use ``model.generate()`` with "
"``apply_chat_template()`` in tokenizer."
msgstr ""
"以前，我们使用 ``model.chat()`` （有关更多详细信息，请参阅先前Qwen模型中的 ``modeling_qwen.py`` "
"）。现在，我们遵循 ``transformers`` 的实践，直接使用 ``model.generate()`` 配合tokenizer中的 "
"``apply_chat_template()`` 方法。"

#: ../../source/getting_started/quickstart.rst:84
#: d350eeb6f4fe4bba8eab83f4c4dbb9ff
msgid ""
"To tackle with downloading issues, we advise you to try with from "
"ModelScope, just changing the first line of code above to the following:"
msgstr "为了解决下载问题，我们建议您尝试从 ModelScope 进行下载，只需将上述代码的第一行更改为以下内容："

#: ../../source/getting_started/quickstart.rst:91
#: ee496adb22e64a308f9235a8ea01d8c7
msgid ""
"Streaming mode for model chat is simple with the help of "
"``TextStreamer``. Below we show you an example of how to use it:"
msgstr "借助 ``TextStreamer`` ，chat的流式模式变得非常简单。下面我们将展示一个如何使用它的示例："

#: ../../source/getting_started/quickstart.rst:107
#: 75646f64a99a44d2bd2ad3d5ab751005
msgid "vLLM for Deployment"
msgstr "使用vLLM部署"

#: ../../source/getting_started/quickstart.rst:109
#: c33f4df8888240df9aac1ba10c272657
msgid ""
"To deploy Qwen2, we advise you to use vLLM. vLLM is a fast and easy-to-"
"use framework for LLM inference and serving. In the following, we "
"demonstrate how to build a OpenAI-API compatible API service with vLLM."
msgstr ""
"要部署Qwen2，我们建议您使用vLLM。vLLM是一个用于LLM推理和服务的快速且易于使用的框架。以下，我们将展示如何使用vLLM构建一个与OpenAI"
" API兼容的API服务。"

#: ../../source/getting_started/quickstart.rst:114
#: 811f37af83d64ad58d29ad241d7fff46
msgid "First, make sure you have installed ``vllm>=0.4.0``:"
msgstr "首先，确保你已经安装 ``vLLM>=0.4.0`` ："

#: ../../source/getting_started/quickstart.rst:120
#: 19cc9723b504403e9128946d8a0b51d8
msgid ""
"Run the following code to build up a vllm service. Here we take Qwen2-7B-"
"Instruct as an example:"
msgstr "运行以下代码以构建vllm服务。此处我们以Qwen2-7B-Instruct为例："

#: ../../source/getting_started/quickstart.rst:127
#: 25ac5cb4cdde4985806e567f63d722ab
msgid ""
"Then, you can use the `create chat interface "
"<https://platform.openai.com/docs/api-"
"reference/chat/completions/create>`__ to communicate with Qwen:"
msgstr ""
"然后，您可以使用 `create chat interface <https://platform.openai.com/docs/api-"
"reference/chat/completions/create>`__ 来与Qwen进行交流："

#: ../../source/getting_started/quickstart.rst:145
#: 7111eef44fff44eb90807ba34d8b4012
msgid ""
"or you can use Python client with ``openai`` Python package as shown "
"below:"
msgstr "或者您可以按照下面所示的方式，使用 ``openai`` Python 包中的 Python 客户端："

#: ../../source/getting_started/quickstart.rst:174
#: 9a8feed0422c4f239a7e435f653b9682
msgid "Next Step"
msgstr "下一步"

#: ../../source/getting_started/quickstart.rst:176
#: eb8d2b9b88cf4066aa7f741fd25d9ad7
msgid ""
"Now, you can have fun with Qwen models. Would love to know more about its"
" usages? Feel free to check other documents in this documentation."
msgstr "现在，您可以尽情探索Qwen模型的各种用途。若想了解更多，请随时查阅本文档中的其他内容。"

