# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#

msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 22:57+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/run_locally/mlx-lm.rst:2 100e966488594715813b66138fee929d
msgid "MLX-LM"
msgstr ""

#: ../../source/run_locally/mlx-lm.rst:4 160cb05f220f465d891b9a236bd440a0
msgid ""
"`mlx-lm <https://github.com/ml-explore/mlx-examples/tree/main/llms>`__ "
"helps you run LLMs locally on Apple Silicon. It is available at MacOS. It"
" has already supported Qwen models and this time, we have also provided "
"checkpoints that you can directly use with it."
msgstr ""
"`mlx-lm <https://github.com/ml-explore/mlx-examples/tree/main/llms>`__ "
"能让你在Apple Silicon上运行大型语言模型，适用于MacOS。``mlx-lm`` 已支持Qwen模型，"
"此次我们提供直接可用的模型文件。"

#: ../../source/run_locally/mlx-lm.rst:9 41cb684fde614a7b964fd9e470ea972b
msgid "Prerequisites"
msgstr "准备工作"

#: ../../source/run_locally/mlx-lm.rst:11 d5f51ef3f02d4987911d7973be86dc28
msgid "The easiest way to get started is to install the ``mlx-lm`` package:"
msgstr "首先需要安装 ``mlx-lm`` 包："

#: ../../source/run_locally/mlx-lm.rst:13 78ec5d3202ad46549ca45c7c60dbd999
msgid "with ``pip``:"
msgstr "使用 ``pip`` ："

#: ../../source/run_locally/mlx-lm.rst:19 49c9ddc9ee9d45e8a1ddf3e61c7abc8d
msgid "with ``conda``:"
msgstr "使用 ``conda`` ："

#: ../../source/run_locally/mlx-lm.rst:27 7cc5f90094b24917a815c6402418f160
msgid "Runnig with Qwen MLX Files"
msgstr "使用Qwen MLX模型文件"

#: ../../source/run_locally/mlx-lm.rst:29 a2ecc23526134e3f991238eaef57e1be
msgid ""
"We provide model checkpoints with ``mlx-lm`` in our Hugging Face "
"organization, and to search for what you need you can search the repo "
"names with ``-MLX``."
msgstr ""
"我们已在Hugging Face提供了适用于 ``mlx-lm`` 的模型文件，请搜索带 ``-MLX`` 的存储库。"

#: ../../source/run_locally/mlx-lm.rst:31 4efb7a29e4614082b98be6e7e467ec3d
msgid ""
"Here provides a code snippet with ``apply_chat_template`` to show you how"
" to load the tokenizer and model and how to generate contents."
msgstr "这里我们展示了一个代码样例，其中使用了 ``apply_chat_template`` 来应用对话模板。"

#: ../../source/run_locally/mlx-lm.rst:54 53f991f7820a456bb366dc03c553b4f6
msgid "Make Your MLX files"
msgstr "自行制作MLX格式模型"

#: ../../source/run_locally/mlx-lm.rst:56 133ff0db1ce94efba0d1ae6ee8a6615e
msgid "You can make mlx files with just one command:"
msgstr "仅用一条命令即可制作mlx格式模型"

#: ../../source/run_locally/mlx-lm.rst:62 e8c0a6a2198342c0893fc6cacbc16362
msgid "where"
msgstr "参数含义分别是"

#: ../../source/run_locally/mlx-lm.rst:64 63447816aeae4f7bacc48eec41505256
msgid "``--hf-path``: the model name on Hugging Face Hub or the local path"
msgstr "``--hf-path``: Hugging Face Hub上的模型名或本地路径"

#: ../../source/run_locally/mlx-lm.rst:66 34b7c6f2f78440ec9d233871719efab3
msgid "``--mlx-path``: the path for output files"
msgstr "``--mlx-path``: 输出模型文件的存储路径"

#: ../../source/run_locally/mlx-lm.rst:68 ba2281a5f27b4f6684e796e49434e3c3
msgid "``-q``: enable quantization"
msgstr "``-q``: 启用量化"

